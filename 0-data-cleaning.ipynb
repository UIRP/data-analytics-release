{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data access and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning forms an important if messy part of preparing real-world data for use in a database or a data analysis process.  Cleaning is intended to standardize data formats and representations into a more legible form factor for the machine.\n",
    "\n",
    "What sorts of things can go wrong when dealing with data?\n",
    "\n",
    "- Data in incorrect format\n",
    "- Inconsistent labels and values\n",
    "- Poorly chosen empty values\n",
    "- Gaps or overlaps in data\n",
    "- Poor quality conversion (as in OCR text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilize weather data for the Coleto Creek Reservoir from 2003–14, provided by the [National Oceanic and Atmospheric Administration (NOAA)](https://www.noaa.gov/).  The data set contains 4,197 daily weather observations of evaporation–transpiration, precipitation, and temperature extremes.\n",
    "\n",
    "![](http://water.weather.gov/ahps2/images/hydrograph_photos/ckdt2/dsc00171.jpg)\n",
    "\n",
    "The data set is organized as a comma-separated value (CSV) file with the fields `LATITUDE`, `LONGITUDE`, `DATE`, `ET`, `PRCP`, `TMAX`, `TMIN`.  Pandas provides a spreadsheet-like or database-like window on the data structure.  We will load the data with the Pandas function `read_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure itself is what Pandas calls a `DataFrame`.  Again, think of a spreadsheet containing columns of data.  These columns are Pandas `Series` and can be accessed by their column header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the observations included in this data set as raw data are incomplete—in other words, the raw data contain missing values and outliers.  These can be troublesome since the format of missing data can vary widely—for instance, `0`, `NaN`, `\"\"`, `NULL`, and `-9999` all occur in practice—and so we commonly need to clean data by either removing entries, standardizing missing values to a useful default, or interpolating values (only if necessary and done carefully!).\n",
    "\n",
    "In this tutorial, we will focus on `TMAX` and `TMIN` data to demonstrate how to clean data of missing values and outliers.  First, let's examine the distribution of `TMAX` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular data set, missing values are represented by `-9999`.  We need to filter out these rows and handle the values in a more transparent way.  We could either replace the values with non-calculating values (like `NaN`), or remove the lines directly.  In this case, we will remove the lines directly, but I'll show you as well how the first approach works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else could need checking in this database for $T_\\textrm{min}$ and $T_\\textrm{max}$?  Let's make sure they are ordered properly.  (Note that we can access a column then a row, or a row then a column.)  Simply find and swap any cases where this occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that gives us the persistent problem that DataFrames don't like to change their values.  Explicitly, if you _mutate_ (change) data, Pandas generates a copy.  Thus the `values` method at the end there.\n",
    "\n",
    "Now plot the histogram of temperature data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what's going on with the scale?  Well, it turns out that the values are stored in tenths of a degree centigrade, which allows them to be stored without a decimal point.  Let's fix that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the correct range and cleaned data values, now examine the time series form:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Why are the values clustered apart from each other?  We'll need to convert that column (`DATE`) to a Python `datetime` representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check the box plot distribution form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will show you how to perform some basic statistical\n",
    "analysis of the dataset.  This will help us diagnose any additional basic features.  As an example, select year 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate the extremes---the hottest and the coldest day in the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the days in which the maximum temperature exceeds 35 °C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data smoothing is an important way to see general patterns\n",
    "associated with a dataset.  Noisy data tend to obscure the visibility of trends.  In this section, we will introduce several data smoothing functions available in Pandas.\n",
    "\n",
    "Some of these rely on the concept of a _rolling window_, in which a sample of several neighboring values is used to determine the behavior at a given point in time.  Others are sampling filters used in signal processing applications.\n",
    "\n",
    "First, let's examine the _rolling mean_ and _rolling standard deviation_.  These are built into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Why are there gaps in the rolling mean?\n",
    "\n",
    "-   How sensitive is the system to the value of `n`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these together to build a very nice profile plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use some low-pass filtering to smooth the data.  (Low-pass filters only pass signal components that are below a threshold frequency.  This helps because noise tends to look like high-frequency variability.)  The Savitsky–Golay filter is popular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we use a [forward–backward filter](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.filtfilt.html) to smooth the data.  (This technique applies a linear filter once forward and once backward to remove offset.  Note that the rolling mean, for instance, tends to lag the signal in time series.)  Since we need to use a filtering algorithm with this, we'll introduce the [Butterworth](https://infogalatic.com/info/Butterworth_filter) [filter](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html#scipy.signal.butter), which is designed to be as \"flat\" as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the original data with each of the smoothed data methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   How can we quantify the \"goodness\" of these smooth sets?  [differentiability,residuals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations Between Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation can help us understand the relationship between different\n",
    "variables.  For variables $x$ and $y$, correlation $\\rho$ can be computered\n",
    "as:\n",
    "\n",
    "$$\n",
    "\\rho_{x,y}\n",
    "=\n",
    "\\frac{E[x-\\mu_x][y-\\mu_y]}{\\sigma_{x}\\sigma_{y}}\n",
    "$$\n",
    "\n",
    "Let's see the correlation of maximum temperature, minimum temperature and E–T:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are willing to install some other packages, there are some nice visualizations available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TMAX` and `TMIN` are clearly correlated with each other, while they are not\n",
    "correlated with `EVAP`.\n",
    "\n",
    "Now we want to see if the `TMAX` of one day is correlated with the `TMAX` of other days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly `TMAX` values become less correlated as time lag increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lessons were developed by Erhu Du, Jane Lee, and Neal Davis for Computational Science and Engineering at the University of Illinois.  Development was supported by a grant from MathWorks, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
