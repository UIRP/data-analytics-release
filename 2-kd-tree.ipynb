{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining:  Unsupervised Exploratory Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-D Tree for Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[$k$-d trees](https://en.wikipedia.org/wiki/K-d_tree) partition $k$-dimensional space in order to organize points and data access.  They are particularly apt for multidimensional key searches.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Kdtree_2d.svg/740px-Kdtree_2d.svg.png)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Tree_0001.svg/740px-Tree_0001.svg.png)\n",
    "\n",
    "There's a sense in which that's a terrible definition, particularly if you're thinking of _physical_ dimensions.  Who needs a 150-dimensional problem?  What we mean by \"dimension,\" though, is \"thing that can differ\" across our dataset:  factors.  Thus you can build the tree for a customer relations database along location, name, company size, frequency of purchase, etc.\n",
    "\n",
    "Why use $k$-d trees?  They are very fast at identifying neighboring points by eliminating large swaths of search space.  Because you know that adjacent points are on adjacent branches, it's relatively efficient to classify points together on the basis of the tree structure.\n",
    "\n",
    "Today we will use a $k$-d tree for identifying similarity of values based on their parameterized description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (10,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Online News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The University of Californiaâ€”Irvine provides [a collection of standard data sets](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) suitable for machine learning and data mining applications.  We will use a data set containing a summary of articles published by [Mashable](https://mashable.com/).  The data file contains 39,644 rows and 61 columns (corresponding to 58 predictive attributes, 2 non-predictive attributes and 1 numerical label about article popularity).  The source website contains more information (about dates, content, etc.).\n",
    "\n",
    "We will use Pandas to represent the data, and SciPy to provide the $k$-d tree data structure and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   How can you check what the column headers are labeled as?  What about the basic range and order-of-magnitude of the entries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transform the original (numerical) labels to binary labels. Each article will be designated as either popular (label is more than median value, indicated by `True`), or not popular (label is less than median value, indicated by `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a classification tree from an original data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a $k$-d tree using SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not a really great tool in Python for visualizing this tree (although generally one may be able to use `pygraphviz`).  A highly-dimensional $k$-d tree doesn't really have a good 2D or 3D representation, though.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/3dtree.png/500px-3dtree.png)\n",
    "\n",
    "You can tune the number of leaves from each node using a second parameter in `spatial.KDTree`, although quantifying the relative efficiency is a drawn-out process.  From the [SciPy documentation](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html):\n",
    "\n",
    "> High-dimensional nearest-neighbor queries are a substantial open problem in computer science.\n",
    "\n",
    "We can construct an array of trees and compare their relative performance for lookups.  First, we will sample a subset of the overall data set for efficiency in timing (since this can be a time-intensive process).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "pop_sample = pop.sample( n=n )\n",
    "\n",
    "leaves = [ 1,2,4,10,20,50,100 ]\n",
    "trees = []\n",
    "for leaf in leaves:\n",
    "    itree = spatial.KDTree( pop_sample.loc[ :,' timedelta':' shares' ],leafsize=leaf )\n",
    "    trees.append( itree )\n",
    "\n",
    "for i,tree in enumerate( trees ):\n",
    "    %timeit tree.query_pairs( 2.0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Do other operations show a similar or opposite trend?\n",
    "\n",
    "$k$-d trees can become unbalanced.  They tend to be more effective tools when you don't know the underlying data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a classification tree from PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the dataset has 53 dimensions.  With PCA, we can reduce this to 27 while maintaining 80% of our accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covMatrix = pop.loc[ :,' timedelta':' shares' ].cov()\n",
    "\n",
    "from numpy.linalg import eig\n",
    "eigV = eig( covMatrix )\n",
    "eigVal = eigV[ 0 ]\n",
    "eigVec = eigV[ 1 ]\n",
    "\n",
    "info = 0.80\n",
    "sortedValue = sorted( eigVal,reverse=True )\n",
    "sortedIndex = sorted( range( len( eigVal ) ), key=lambda k:eigVal[ k ],reverse=True)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot( range( 1,len( sortedValue )+1 ),np.cumsum( sortedValue ) / np.sum( sortedValue ),'mo--',linewidth=2 )\n",
    "ax.plot( range( 1,len( sortedValue )+1),np.ones( len( sortedValue ) )*info,'b-' )\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Percentage of information (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out the indices corresponding to the eigenvalues required to hit this information carrying capacity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigIDs = []\n",
    "for i in range( 0,len( sortedValue ) ):\n",
    "    if sum( sortedValue[ 0:i+1 ] ) / sum( sortedValue ) > info:\n",
    "        eigIDs = sortedIndex[ 0:i+1 ]\n",
    "        break\n",
    "\n",
    "eigVal_pca = eigVal[ eigIDs ]\n",
    "eigVec_pca = eigVec[ :,eigIDs ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the PCA eigenvectors to produce the reduced-dimensional data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = pop.loc[ :,' timedelta':' shares' ].as_matrix()\n",
    "data_pca = data_array.dot( eigVec_pca )\n",
    "plt.plot( data_pca[ :,0 ],data_pca[ :,1 ],'bo' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, construct the $k$-d tree from the PCA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tree = spatial.KDTree( data_pca )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify how much \"better\" this tree is, we can carry out time trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pca_tree.query_ball_point( np.ones((1,27))*True,5 )\n",
    "%timeit tree.query_ball_point( np.ones((1,27))*True,5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or figure out the relative error rate.\n",
    "\n",
    "-   How could we measure the error rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   K. Fernandes, P. Vinagre and P. Cortez.  (2015)  A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. *Proceedings of the 17th EPIA 2015 (Portuguese Conference on Artificial Intelligence)*.  Coimbra, Portugal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lessons were developed by Erhu Du, Jane Lee, and Neal Davis for Computational Science and Engineering at the University of Illinois.  Development was supported by a grant from MathWorks, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
